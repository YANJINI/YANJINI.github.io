I"ª<p>During the first class of my advanced Econometrics course at graduate school, I volunteered to solve some exercise problems that involved proving certain estimators were unbiased and consistent mathematically. While proving whether estimators were unbiased was trivial, demonstrating consistency was more challenging. However, I remembered from my self-study in Econometrics that if the variance of the estimator converges to zero probabilistically, we can conclude that the estimator is consistent. The proof is closely related to Chebyshevâ€™s Inequality, but I had difficulty formulating it during the class. After the class, I revisited <a href="(https://blog.naver.com/boadoboado11/222684138583)">an old post</a> I had written about Chebyshevâ€™s Inequality in Korean from a year ago and decided to write this post.</p>

<h2 id="chebyshevs-inequality">Chebyshevâ€™s Inequality</h2>
<p>Chebyshevâ€™s Inequailty is fomulated as follows.</p>

<p>$P(X \geq a) \leq \dfrac{E(X)}{a} \quad \forall_{X} \geq 0, \forall_a \geq 0 \tag{1}$</p>

<p>Here $X$ is a random variable with a finite mean and variance, and $a$ is a constant. The inequality always holds regardless of the distribution of $X$.</p>

<p>The proof is quite straightforward.</p>

<p>When there is a continuous random variable $X$ and $f(X)$ is a probability density function of $X$,</p>

<p>$E(X) = \displaystyle{\int_{0}^{\infty}} Xf(X)dx \geq P(X \geq a) = \displayst\tag{2}$</p>

:ET