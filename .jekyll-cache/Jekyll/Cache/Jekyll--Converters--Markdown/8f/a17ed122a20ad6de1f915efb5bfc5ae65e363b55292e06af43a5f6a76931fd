I"<p>During the first class of my advanced Econometrics course at graduate school, I volunteered to solve some exercise problems that involved proving certain estimators were unbiased and consistent mathematically. While proving whether estimators were unbiased was trivial, demonstrating consistency was more challenging. However, I remembered from my self-study in Econometrics that if the variance of the estimator converges to zero probabilistically, we can conclude that the estimator is consistent. The proof is closely related to Chebyshev’s Inequality, but I had difficulty formulating it during the class. After the class, I revisited <a href="(https://blog.naver.com/boadoboado11/222684138583)">an old post</a> I had written about Chebyshev’s Inequality in Korean from a year ago and decided to write this post.</p>

<h2 id="chebyshevs-inequality">Chebyshev’s Inequality</h2>
<p>Chebyshev’s Inequailty is fomulated as follows.</p>

<p>$P(X \geq a) \leq \dfrac{E(X)}{a} \quad \forall_{X} \geq 0, \forall_a \geq 0 \tag{1}$</p>

<p>Here $X$ is a random variable and a is a constant. The inequality relationship between the random variable and the constant always holds no matter what distribution $X$ follows.</p>

<p>Yes, it is correct to say that Chebyshev’s inequality is formulated as follows:</p>

<p>$P(X \geq a) \leq \dfrac{E(X)}{a} \quad \forall_{X} \geq 0, \forall_a \geq 0 \tag{1}$</p>

<p>This inequality states that the probability that a random variable $X$ deviates from its expected value $E(X)$ by more than a certain constant $a$ is bounded by the ratio of the variance of $X$ to the square of $a$.</p>

<p>It is also correct to state that this inequality holds true for any non-negative random variable $X$ and any positive constant $a$, regardless of the distribution of $X$.</p>
:ET