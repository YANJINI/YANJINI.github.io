I"(
<p>During the first class of my advanced Econometrics course at graduate school, I volunteered to solve some exercise problems that involved proving certain estimators were unbiased and consistent mathematically. While proving whether estimators were unbiased was trivial, demonstrating consistency was more challenging. However, I remembered from my self-study in Econometrics that if the variance of the estimator converges to zero probabilistically, we can conclude that the estimator is consistent. The proof is closely related to Markov’s Inequality, but I had difficulty formulating it during the class. After the class, I revisited <a href="(https://blog.naver.com/boadoboado11/222684138583)">an old post</a> I had written about Markov’s Inequality in Korean from a year ago and decided to write this post.</p>

<h2 id="markovs-inequality">Markov’s Inequality</h2>
<p>Markov’s Inequailty is fomulated as follows.</p>

<p>$P(X \geq a) \leq \dfrac{E(X)}{a} \quad \forall_X \geq 0, \forall_a \geq 0 \tag{1}$</p>

<p>Here $X$ is a positive random variable with a finite mean and variance, and $a$ is a positive constant. The inequality always holds regardless of the distribution of $X$.</p>

<p>The proof is quite straightforward.</p>

<p>When there is a continuous random variable $X$ and $f(X)$ is a probability density function of $X$,</p>

\[\begin{align}
E(X) 
&amp;= \displaystyle{\int_{0}^{\infty}} xf(x)dx\\
&amp;= \displaystyle{\int_{0}^{a}} xf(x)dx + \displaystyle{\int_{a}^{\infty}} xf(x)dx\\
&amp;\geq \displaystyle{\int_{a}^{\infty}} xf(x)dx\\
&amp;\geq \displaystyle{\int_{a}^{\infty}} af(x)dx = aP(X \geq a) \quad \because x \geq a
\end{align} \tag{2}\]

<p>It is easy to show the Equation $1$ holds by deviding the both sides of the Equation $2$.</p>

<p>We can use this inequality to prove consistency of estimators.</p>

<p>The mathematical definition of consistency is as follows.</p>

<table>
  <tbody>
    <tr>
      <td>$P\left(</td>
      <td>X-E(X)</td>
      <td>\geq \epsilon \right) \xrightarrow{N \to \infty} 0 \tag{3}$</td>
    </tr>
  </tbody>
</table>

<p>Let’s square both sides and apply the Markov’s Inequality</p>

<p>$$</p>

<h2 id="proof-of-consistency">Proof of consistency</h2>
<p>The two estimators worked on the class were the following.</p>

<p>For $iid$ random variable $X_{i}$ from the normal distribution with mean $\mu$ and variance $\sigma^{2}$,</p>

<p>$\hat{\theta}_{N} = \dfrac{1}{N} + \dfrac{1}{N} \sum_{i=1}^{N} X_{i} \tag{3}$</p>

<p>$\hat{\theta}_{N} = X_{N} \tag{4}$</p>

<p>$X_{N}$ is the $N$-th observation in our sample.</p>

:ET