<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-03-27T21:49:15+02:00</updated><id>http://localhost:4000/feed.xml</id><entry><title type="html">Why Differentiation for Optimization?</title><link href="http://localhost:4000/blog/2023/03/26/Why-differentiate-for-optimization/" rel="alternate" type="text/html" title="Why Differentiation for Optimization?" /><published>2023-03-26T00:00:00+01:00</published><updated>2023-03-26T00:00:00+01:00</updated><id>http://localhost:4000/blog/2023/03/26/Why-differentiate-for-optimization</id><content type="html" xml:base="http://localhost:4000/blog/2023/03/26/Why-differentiate-for-optimization/"><![CDATA[<h2 id="taylors-series">Taylor’s Series</h2>
<p>Taylor’s Series is a useful mathematical tool for approximating an unknown function with polynomial terms around a specific point. Let $f(x)$ be an infinitely-differentiable function of a variable $x$ and $a$ be the point around which we want to approximate the function. The function can be represented by the linear combination of the polynomials as follows.</p>

<p>$f(x) = c_0+c_{1}(x-a)+c_{2}(x-a)^{2}+c_{3}(x-a)^{3}+…\tag{1}$</p>

<p>Here, we subtract the constant $a$ from all polynomial terms for notational convenience. We can find the coefficients for the polynomials $c_0, c_1, c_2,$ and so on, by taking the values of the derivatives of both sides at the point $x=a$ as follows.</p>

\[\begin{align}
f(a) &amp;= c_{0}\\
f^{'}(a) &amp;= 1! \cdot c_{1}\\
f^{''}(a) &amp;= 2! \cdot c_{2}\\
f^{'''}(a) &amp;= 3! \cdot c_{3}\\
&amp; \vdots
\end{align} \tag{2}\]

<p>Then we can rewrite Equation $1$ as follows.</p>

\[\begin{align}
f(x) &amp;= f(a) + \dfrac{f^{'}(a)}{1!}(x-a) + \dfrac{f^{''}(a)}{2!}(x-a)^{2} + \dfrac{f^{'''}(a)}{3!}(x-a)^{3} + ...\\
&amp;= \sum^{\infty}_{i=0}\frac{f^i(a)}{i!}(x-a)^i
\end{align} \tag{3}\]

<p>Here, $f^{(i)}$ denotes the $i$-th order derivative of $f$, and $f^{(0)} = f$. The approximation is accurate when $x$ is close enough to $a$. The plot of the approximations of the log function around $x=1$ below shows this property well. The approximations get poorer as $x$ gets away from $1$.</p>

<p><a id="Figure-1"></a></p>

<div class="figure">
    <img src="/image/why_differentiation/log_taylor.png" style="width: 80%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 1. Taylor approximations of log function around $x=1$.</span> 
    </div>
</div>

<p>See <a href="#code-1">A1</a> for the code to generate this figure.</p>

<p>We can further generalize Equation $3$ by setting $x = a+h$, where $h$ is a small enough constant.</p>

\[\begin{align}
f(a+h) &amp;= f(a) + \dfrac{f^{'}(a)}{1!}h + \dfrac{f^{''}(a)}{2!}h^{2} + \dfrac{f^{'''}(a)}{3!}h^{3} + ...\\
&amp;= \sum^{\infty}_{i=0}\frac{f^i(a)}{i!}h^i
\end{align} \tag{4}\]

<p>Equation $4$ is useful because it explicitly shows how the output changes as the input varies between $a+h$ and $a$, which can be calculated as $f(a+h) - f(a)$. This is fundamental for numerical optimization, where we try to find the optimal value of an unknown function given only input-output pairs. But this Equation $4$ is also an important tool for deriving First Order Condition (FOC) and Complete Second Order Condition (CSOC).</p>

<h2 id="first-order-condition-foc">First Order Condition (FOC)</h2>

<h2 id="second-order-condition-soc">Second Order Condition (SOC)</h2>

<h2 id="complete-second-order-condition-csoc">Complete Second Order Condition (CSOC)</h2>

<h2 id="appendix">Appendix</h2>

<p><a id="code-1"></a></p>
<h3 id="a1-plot-taylor-approximations-of-log">A1: Plot Taylor approximations of log</h3>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># plot log function</span><span class="w">
</span><span class="n">xx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.01</span><span class="p">)</span><span class="w">
</span><span class="n">yy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span><span class="w">

</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span><span class="w"> </span><span class="n">yy</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"l"</span><span class="p">,</span><span class="w">
     </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"x"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"log(x)"</span><span class="p">,</span><span class="w"> 
     </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"The log function"</span><span class="p">,</span><span class="w"> 
     </span><span class="n">ylim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w">
     </span><span class="n">xlim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">))</span><span class="w">



</span><span class="c1"># plot Talor approximations of log function around x = 1</span><span class="w">
</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="n">hh</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.01</span><span class="p">)</span><span class="w">

</span><span class="n">first_ord</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">^</span><span class="p">(</span><span class="m">-1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hh</span><span class="w">
</span><span class="n">second_ord</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">first_ord</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">factorial</span><span class="p">(</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">^</span><span class="p">(</span><span class="m">-2</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hh</span><span class="o">^</span><span class="m">2</span><span class="w">
</span><span class="n">third_ord</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">second_ord</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">factorial</span><span class="p">(</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="o">*</span><span class="n">x</span><span class="o">^</span><span class="p">(</span><span class="m">-3</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hh</span><span class="o">^</span><span class="m">3</span><span class="w">
</span><span class="n">fourth_ord</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">third_ord</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">factorial</span><span class="p">(</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="m">6</span><span class="p">)</span><span class="o">*</span><span class="n">x</span><span class="o">^</span><span class="p">(</span><span class="m">-4</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hh</span><span class="o">^</span><span class="m">4</span><span class="w">

</span><span class="n">lines</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">hh</span><span class="p">,</span><span class="w"> </span><span class="n">first_ord</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">hh</span><span class="p">,</span><span class="w"> </span><span class="n">second_ord</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w"> 
</span><span class="n">lines</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">hh</span><span class="p">,</span><span class="w"> </span><span class="n">third_ord</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"orange"</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">hh</span><span class="p">,</span><span class="w"> </span><span class="n">fourth_ord</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"purple"</span><span class="p">)</span><span class="w">

</span><span class="n">legend</span><span class="p">(</span><span class="s2">"topleft"</span><span class="p">,</span><span class="w"> </span><span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"Log function"</span><span class="p">,</span><span class="w"> </span><span class="s2">"First order"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Second order"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Third order"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Fourth order"</span><span class="p">),</span><span class="w"> 
       </span><span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">,</span><span class="w"> </span><span class="s2">"orange"</span><span class="p">,</span><span class="w"> </span><span class="s2">"purple"</span><span class="p">),</span><span class="w"> </span><span class="n">lty</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w">
  

</span></code></pre></div></div>]]></content><author><name></name></author><category term="all" /><category term="opt" /><summary type="html"><![CDATA[Taylor’s Series Taylor’s Series is a useful mathematical tool for approximating an unknown function with polynomial terms around a specific point. Let $f(x)$ be an infinitely-differentiable function of a variable $x$ and $a$ be the point around which we want to approximate the function. The function can be represented by the linear combination of the polynomials as follows.]]></summary></entry><entry><title type="html">Proof of Consistency using Markov’s Inequality</title><link href="http://localhost:4000/blog/2023/03/25/consistency-Chebyshev-inequality/" rel="alternate" type="text/html" title="Proof of Consistency using Markov’s Inequality" /><published>2023-03-25T00:00:00+01:00</published><updated>2023-03-25T00:00:00+01:00</updated><id>http://localhost:4000/blog/2023/03/25/consistency-Chebyshev-inequality</id><content type="html" xml:base="http://localhost:4000/blog/2023/03/25/consistency-Chebyshev-inequality/"><![CDATA[<p>During the first class of my advanced Econometrics course at graduate school, I volunteered to solve some exercise problems that involved proving certain estimators were unbiased and consistent mathematically. While proving whether estimators were unbiased was trivial, demonstrating consistency was more challenging. However, I remembered from my self-study in Econometrics that if the variance of the estimator converges to zero probabilistically, we can conclude that the estimator is consistent. The proof is closely related to Markov’s inequality, but I had difficulty formulating it during the class. After the class, I revisited <a href="(https://blog.naver.com/boadoboado11/222684138583)">an old post</a> I had written about Markov’s inequality in Korean from a year ago and decided to write this post.</p>

<h2 id="markovs-inequality">Markov’s inequality</h2>
<p>Markov’s Inequailty is fomulated as follows.</p>

<p>$P(X \geq a) \leq \dfrac{E(X)}{a} \quad \forall_X \geq 0, \forall_a \geq 0 \tag{1}$</p>

<p>Here $X$ is a non-negative random variable with a finite mean and variance, and $a$ is a non-negative constant. The inequality always holds regardless of the distribution of $X$. The proof is quite straightforward.</p>

<p>When assuming $X$ is continuous and $f(X)$ is a probability density function of $X$,</p>

\[\begin{align}
E(X)
&amp;= \displaystyle{\int_{0}^{\infty}} xf(x)dx\\
&amp;= \displaystyle{\int_{0}^{a}} xf(x)dx + \displaystyle{\int_{a}^{\infty}} xf(x)dx\\
&amp;\geq \displaystyle{\int_{a}^{\infty}} xf(x)dx\\
&amp;\geq \displaystyle{\int_{a}^{\infty}} af(x)dx = aP(X \geq a) \quad \because x \geq a
\end{align} \tag{2}\]

<p>It is easy to show the Equation $1$ holds by dividing the both sides of the Equation $2$ by $a$. We can use this inequality to prove consistency of estimators. Consistency of estimators means an estimator probabilistically converges to its true parameter as the sample size goes to infinity. The mathematical definition of consistency is as follows.</p>

<p>When $\hat{\theta}_{N}$ is an estimator with sample size of $N$ and the true parameter is $\theta$,</p>

<p>$P\left(|\hat{\theta}_{N} - \theta| \geq \epsilon \right) \xrightarrow{N \to \infty} 0 \tag{3}$</p>

<p>Here $\epsilon$ is any positive number. Let’s square both sides and apply the Markov’s inequality.</p>

\[\begin{align}
P\left((\hat{\theta}_{N} - \theta)^{2} \geq \epsilon^{2} \right) 
&amp;\leq \dfrac{E\left[ (\hat{\theta}_{N} - \theta)^{2} \right]}{\epsilon^{2}}\\
&amp;\leq \dfrac{Var(\hat{\theta}_{N})}{\epsilon^{2}}
\end{align} \tag{4}\]

<p>Note that, although $\hat{\theta}$ is a random variable that may take negative values, $(\hat{\theta}_{N} - \theta)^{2}$ is always non-negative. Therefore, we can apply Markov’s inequality. $E\left[ (\hat{\theta}_{N} - \theta)^{2} \right]$ on the right-hand side in Equation $4$ is just a variance notation when $\hat{\theta}_{N}$ is asymptotically unbiased, which means when $E(\hat{\theta}_{N}) \xrightarrow{N \to \infty} \theta$. This is because the asymptotic relationship below holds.</p>

<p>$Var\left(\hat{\theta}_{N}\right) = E\left[(\hat{\theta}_{N} - E(\hat{\theta}_{N}))^{2}\right] \xrightarrow{N \to \infty} E\left[ (\hat{\theta}_{N} - \theta)^{2} \right] \tag{5}$</p>

<p>So, if we show the variance of the estimator converges to zero as $N \to \infty$, we can say that the estimator is consistent.</p>

<h2 id="proof-of-consistency">Proof of consistency</h2>
<p>For $iid$ random variable $X_{i}$ from the normal distribution with mean $\mu$ and variance $\sigma^{2}$, let the following $\hat{\theta}_{N}$ be an estimator for $\mu$.</p>

\[\hat{\theta}_{N} = \dfrac{1}{N} + \dfrac{1}{N} \sum_{i=1}^{N} X_{i} \tag{6}\]

<p>This estimator is biased, but asymptotically unbiased as follows.</p>

\[\begin{align}
E(\hat{\theta}_{N})
&amp;= E\left(\dfrac{1}{N}\right) + E\left(\dfrac{1}{N}\sum_{i=1}^{N}X_{i}\right)\\
&amp;= \dfrac{1}{N} + \dfrac{1}{N}N\mu\\
&amp;= \dfrac{1}{N} + \mu \xrightarrow{N \to \infty} \mu
\end{align} \tag{7}\]

<p>Let’s calculate the variance of the estimator.</p>

\[\begin{align}
Var\left(\hat{\theta}_{N}\right) 
&amp;= Var\left(\dfrac{1}{N}\right) + Var\left(\dfrac{1}{N} \sum_{i=1}^{N}X\_{i}\right)\\
&amp;= \dfrac{1}{N^{2}} \sum_{i=1}^{N} Var(X_i) \quad \because iid\\
&amp;= \dfrac{1}{N}\sigma^{2}
\end{align} \tag{8}\]

<p>The variance definitely converges to zero when $N \to \infty$.</p>

\[\begin{align}
P\left(|\hat{\theta}_{N} - \mu| \geq \epsilon \right) 
&amp;= P\left((\hat{\theta}_{N} - \mu)^{2} \geq \epsilon^{2} \right)\\
&amp;\leq \dfrac{E\left[ (\hat{\theta}_{N} - \mu)^{2} \right]}{\epsilon^{2}} = \dfrac{Var\left(\hat{\theta}_{N}\right)}{\epsilon^{2}}

\xrightarrow{N \to \infty} 0 
\end{align} \tag{9}\]

<p>So, we can conclude that the estimator is a biased, but consistent estimator.</p>]]></content><author><name></name></author><category term="all" /><category term="econ" /><summary type="html"><![CDATA[During the first class of my advanced Econometrics course at graduate school, I volunteered to solve some exercise problems that involved proving certain estimators were unbiased and consistent mathematically. While proving whether estimators were unbiased was trivial, demonstrating consistency was more challenging. However, I remembered from my self-study in Econometrics that if the variance of the estimator converges to zero probabilistically, we can conclude that the estimator is consistent. The proof is closely related to Markov’s inequality, but I had difficulty formulating it during the class. After the class, I revisited an old post I had written about Markov’s inequality in Korean from a year ago and decided to write this post.]]></summary></entry><entry><title type="html">How Risky is McDonald’s Stock?</title><link href="http://localhost:4000/blog/2023/02/26/Mcdonald's-stock-VaR/" rel="alternate" type="text/html" title="How Risky is McDonald’s Stock?" /><published>2023-02-26T00:00:00+01:00</published><updated>2023-02-26T00:00:00+01:00</updated><id>http://localhost:4000/blog/2023/02/26/Mcdonald&apos;s-stock-VaR</id><content type="html" xml:base="http://localhost:4000/blog/2023/02/26/Mcdonald&apos;s-stock-VaR/"><![CDATA[<p>In finance, Value at Risk (VaR) is a crucial metric for assessing the risk of an investment portfolio. VaR provides an estimate of rare but potential loss an investment may face over a specific time horizon and confidence level. To calculate VaR, it’s necessary to have an understanding of the underlying distribution of investment returns.</p>

<p>To accurately determine this distribution, various models from time series analysis can be used. The GARCH model, in particular, is known for its effectiveness in analyzing financial data. One of the main reasons to choose GARCH over other models, such as empirical distribution (actually this is even model-free), random walk, or ARIMA, is that it can capture the volatility dynamics of the stock return, which is essential for accurate VaR calculation of financial asset return.</p>

<p>All models in time series analysis aim to explain future values of a time series using its past values, or lags. ACF and PACF are graphical tools used in time series analysis to identify the autocorrelation and partial autocorrelation at various lags of a time series, respectively. They are commonly used to determine the appropriate orders of the GARCH model by examining the decay of the autocorrelation and partial autocorrelation functions.</p>

<p>In this posting, I present the complete process for calculating daily VaR of McDonal’s stock including all the points mentioned above.</p>

<h2 id="prices-simple-returns-and-log-returns">Prices, simple returns and log returns</h2>
<p>Whose distribution exactly do we want to figure out for VaR calculation?  Long story short, we identify the distribution of log returns of McDonald’s stock due to the desirable characteristics of log returns for statistical analysis purpose.</p>

<p>The terms “prices”, “simple returns” and “log returns” refer to different ways of measuring the performance of an investment over time. Price is the most common form of financial asset data we can encounter and the other two are transformed version of this. Let $P_{t}$, $R_{t}$ and $r_{t}$ respectively denote the price, one-period simple return and one-period log return of McDonald’s stock at time $t$. Then $R_{t}$ and $r_{t}$ are defined as</p>

<p>$R_{t}=\dfrac{P_{t}-P_{t-1}}{P_{t-1}}\tag{1}$</p>

<p>$r_{t}=log \left(\dfrac{P_{t}}{P_{t-1}}\right)=log(1+R_{t})\tag{2}$</p>

<p>The log return is frequently referred to as the continuously compounded return on account of the following mathematical principle.</p>

\[\begin{align}
&amp; e^{r_{t}}=\dfrac{P_{t}}{P_{t-1}}\\
&amp; P_{t}=P_{t-1}e^{r_{t}}
\end{align} \tag{3}\]

<p>Both  $R_{t}$ and $r_t$ are measure of the relative movements of price, which is a useful property for comparison of performance between assets. Another technical reason we might be more interested in simple return and log return than raw price is that they could be used as a remedy for non-stationarity of price time-series (differencing).</p>

<p><a id="Figure-1"></a></p>

<div class="figure">
    <img src="/image/VaR/McDonald's price, return and log return.png" style="width: 100%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 1.</span> Closing prices (top), simple returns (middle) and log returns (bottom) of McDonald's stock for past five years.
    </div>
</div>

<p>See <a href="#code-1">A1</a> for the code to generate this figure.</p>

<p>As seen on the chart of McDonald’s stock price, it is impossible to conduct a statistical analysis with this due to the non-stationarity.</p>

<p>The substantial similarity simple return and log return charts exhibit is linked with the fact that the simple return metric is an approximation for  log return by a first-order Taylor expansion. The first order Taylor Expansion of $log(x)$ around $x=1$ is as follows.</p>

\[\begin{align}
log(x)
&amp;\simeq log(1)+log^{'}(1)(x-1)\\
&amp;\simeq 0+\dfrac{1}{1}(x-1)
\end{align} \tag{4}\]

<p>So, when $\dfrac{P_{t}}{P_{t-1}}\simeq 1$</p>

\[\begin{align}
r_{t}=log \left(\dfrac{P_{t}}{P_{t-1}}\right)
&amp;\simeq \dfrac{P_{t}}{P_{t-1}}-1\\
&amp;\simeq \dfrac{P_{t}-P_{t-1}}{P_{t-1}}=R_{t}
\end{align} \tag{5}\]

<p>When $R_{t}$ is small enough, the difference between simple return and log return is negligible. Log return, however, is often chosen over simple return at least for two practical reasons.</p>

<p>First, log returns are additive, which makes it simpler to calculate the total return of an investment over time. Let $r_{k,\ t}$ denote log return over $k$ period at time $t$, then it is defined as</p>

\[\begin{align}
r_{k,\ t}
&amp;=log \left(\dfrac{P_{t}}{P_{t-k}}\right)\\
&amp;=log \left(\dfrac{P_{t}}{P_{t-1}}\right)+log \left(\dfrac{P_{t}}{P_{t-2}}\right)+\ ...\ +log \left(\dfrac{P_{t}}{P_{t-k}}\right)\\
&amp;=r_{1,\ t}+r_{1,\ t-1}+\ ...\ +r_{1,\ t-k}\\
&amp;=r_{t}+r_{t-1}+\ ...\ +r_{t-k}
\end{align} \tag{6}\]

<p>Second, simple returns are inherently asymmetric due to the limited liability issue (simple returns are bounded by $-100\%$) and positive skewness, and logarithmic transformation remedies this problem by mapping the range of $(-1, 0)$ to the range of $(-\infty, 0)$ and limiting big positive values.</p>

<p>To summarize, log returns are often preferred in the statistical analysis of financial assets due to their suitability for comparison, symmetric nature, and ease of calculation for different time horizons. So, I will figure out the distribution of the log returns of McDonald’s stock for calculating Value at Risk (VaR).</p>

<h2 id="value-at-risk-var">Value at Risk (VaR)</h2>
<p>Value at Risk (VaR) is the most broadly used risk metric in finance and is defined as follows.</p>

<p>$P(r_{k,\ t} \leq VaR_{k;\ 1-\alpha})=\alpha \tag{7}$</p>

<p>In plain English, equation $7$ states that the probability of an asset’s return over a given $k$ period, $r_{k,\ t}$ being less than or equal to the value of the VaR for that $k$ period at a confidence level of $1-\alpha$ is equal to $\alpha$. In other words, the VaR represents the maximum expected loss over a $k$ period with a confidence level of $1-\alpha$, or we have $\alpha$ probability of the expected loss going beyond the VaR. The most common values for $\alpha$ are $1\%$ and $5\%$, which are also universal values for significance level in hypothesis testing.</p>

<div class="figure">
    <img src="/image/VaR/Visualization of empirical VaR of McDonald's stock.png" style="width: 65%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 2.</span>The empirical one period (daily) VaR of log returns of McDonald's stock with confidence level of $95\%$ is $-3.05\%$. In mathematical notation, $P(r_{t} \leq VaR_{95\%})=5\%$
    </div>
</div>

<p>See <a href="#code-2">A2</a> for the code to generate this figure.</p>

<p>What we are trying to figure out with VaR is also called “tail risk” and why it goes by the name is intuitively visualized on Figure $2$. The term “tail risk” is used to describe the potential for losses or extreme events that fall outside the normal range of outcomes and occur in the tails of a probability distribution.</p>

<p>The histogram drawn on Figure $2$ is like an empirical distribution, which is a probability distribution based on historical data rather than a theoretical model, which is why it is also called “model-free” distribution. The main assumption behind the choice to use empirical distribution for VaR calculation is that the probability distribution of returns in the past is a good representation of the probability distribution of returns in the future. This is not always valid, especially when the market is showing high volatility. Accurately characterizing the distribution of the data is crucial for VaR calculation, as the choice of distribution significantly impacts the accuracy of the calculated VaR. The issue poses the question, “What is the appropriate model that can accurately reflect the underlying distribution of the data?”</p>

<h2 id="garch-model-with-t-distribution">GARCH model with $t$-distribution</h2>
<p>GARCH stands for Generalized AutoRegressive Conditional Heteroskedasticity, in which the conditional variance of the data is modeled against both autoregressive (AR) process and moving average (MA) process (Equation $9$, respectively blue and red). Let $X_{t}$ denote the time-series (in our case, log returns of McDonald’s stock), then we can assume the following GRACH $(p, q)$ structure.</p>

<p>$X_{t}=\mu+E_{t}=\mu+\sigma_{t}W_{t} \tag{8}$</p>

<p>$\sigma_{t}^{2} = \alpha_0 + \color{#5268DE}{\sum_{i=1}^{p}{\alpha_i E_{t-i}^{2}}} + \color{#DE5252}{\sum_{i=1}^{q}{\beta_i \sigma_{t-i}^2}} \tag{9}$</p>

<p>$W_{t} \sim t_{v}\left(0, \dfrac{v}{v-2}\right) \tag{10}$</p>

<p>$\mu$ is the constant conditional expectation and $E_{t}$ is an error term, which is a multiplication of the conditional variance, $\sigma_{t}^{2}$ and a White Noise innovation, $W_{t}$. The conditional variance, $\sigma_{t}^{2}$, has a GRACH $(p, q)$ structure in Equation $9$. $W_{t}$ is a random $iid$ variable from a standardized Student’s $t$-distribution with $v$ degrees of freedom, $t_{v}\left(0, \dfrac{v}{v-2}\right)$ or simply $t_{v}$.</p>

<p>We often use $t$-distribution as one of these so-called “heavy-tailed” distributions to incorporate the heavy-tailed property of the financial returns instead of the commonly used standard Gaussian distribution. Since Gaussian PDF decays rapidly with $e^{-x^{2}}$, $99\%$ observations appear within the range of $\pm \ 3$ standard deviation on Gaussian distribution, which is thin-tailed. On the other hand, financial returns often show heavy tails. See the R code and result below to calculate how many standard deviations the worst log return of McDonald’s stock for the past five years is from its mean.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">)</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">sd</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="m">-11.4192</span><span class="w">
</span></code></pre></div></div>

<p>GARCH models can be seen as an extension of the ARCH model that only considers AR process. AR process captures how past shocks, $E^{2}_{t-i}$, affect current level of volatility, $\sigma^{2}_{t}$ and MA process captures the impact of past volatility, $\sigma^{2}_{t-i}$ on current volatility, $\sigma^{2}_{t}$. The order of GARCH is commonly defined with $p$ and $q$, which are respectively the order of AR process and the order of MA process. An easy way to think of GARCH model is that GARCH model could be thought of as an ARMA model for the squared error term, $E_{t}^{2}$, but the squared error term replaced with conditional variance, $\sigma_{t}^{2}$. This is why the MA terms in GARCH model are actually autoregrresive, which could be very confusing.</p>

<p>GARCH model is very useful to consider some common features of financial data that we could also find visually from the bottom two plots in <a href="#Figure-1">Figure $1$</a>. They are $1)$ nearly uncorrelated (log) returns with a mean close to zero, $2)$ clusters of volatility and $3)$ some extreme outliers. Our GARCH model with $t$-distribution incorporates these features by $1)$ including a constant mean (not varying conditional expectation), $2)$ applying GARCH $(p, q)$ structure to conditional variance and $3)$ assuming $t$-distribution for the White Noise innovations. A constant mean makes sense because varying conditional expectation implies yes to such a question as “Is it possible to predict the expected stock price tomorrow given data up to today?”, which is empirically proven nearly impossible.</p>

<h2 id="acfpacf-and-garch">ACF/PACF and GARCH</h2>
<p>We can gain insights into the orders of a GARCH model, denoted by $(p, q)$, by analyzing the results of the autocorrelation function and partial autocorrelation function of squared log returns. The autocorrelation function (ACF) and partial autocorrelation function (PACF) are tools used to analyze the patterns of correlation between a time series and its lagged values. Let $Z_{t}$ denote a time series at time $t$, then the ACF and PACF at lag $k$ are defined as</p>

<p>$ACF(k)=Corr(Z_{t}, Z_{t-k}) \tag{11}$</p>

<p>$PACF(k)=Corr(Z_{t}, Z_{t-k} | Z_{t-1}, Z_{t-2}, …, Z_{t-k+1}) \tag{12}$</p>

<p>The ACF measures the correlation between a time series and its lagged values, without considering the effect of intermediate lags. The PACF, on the other hand, measures the same thing after removing the effect of all lags between 1 and $k-1$. The PACF does so by taking a conditional correlation to hold constant the effects of all intervening lags. This is useful in identifying the direct relationship between the time series and its lagged values.</p>

<p>Our purpose in using ACF and PACF is to examine the dependence of the current conditional variance of the log returns, $\sigma_{t}^{2}$, on the squared lagged error terms, $E_{t-j}^{2}$, as well as on the lagged conditional variance, $\sigma_{t-j}^{2}$. So we can use the squared log returns as input for ACF and PACF to account for the squared effect of $E_{t}$ and $\sigma_{t}$ in Equation $8$.</p>

<div class="figure">
    <img src="/image/VaR/ACF, PACF of log returns and squared log returns.png" style="width: 100%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 3.</span>The horizontal blue dotted lines represent the 95% confidence interval. When the (partial) correlation value exceeds these bounds, it suggests a significant (partial) correlation between the current value and the corresponding lag. The log returns appear to have weak or no serial autocorrelation, which makes the constant mean, $\mu$ in Equation $8$ a reasonable choice. On the other hand, the squared log returns show significant serial autocorrelation, which justifies the varying conditional variance, $\sigma^{2}_{t}$ in Equation $9$.
    </div>
</div>

<p>See <a href="#code-3">A3</a> for the code to generate this figure.</p>

<p>It is not so easy to determine the orders of a GARCH model by looking at the results of ACF and PACF in Figure $3$, since they show significant (partial) correlations at multiple lags without clear decay. In such cases, it is standard to start with a simple model, GARCH $(1, 1)$, and examine the dependence structure of the squared standardized residuals from the fitted GARCH model using ACF and PACF plots. The standardized residual from GARCH model is the estimated White Noise term, $\hat{W_{t}}$ and is defined as</p>

<p>$\hat{W_{t}} = \dfrac{E_{t}}{\hat{\sigma}_{t}} \tag{13}$</p>

<p>When the square of these show a significant dependence structure, it indicates that the model is not capturing all the dynamics of the time series. In this case, we can consider increasing the order of the GARCH model and refitting it to the data until the dependence structure becomes not significant.</p>

<p>The conditional variance, $\sigma_{t}^{2}$, from GARCH $(1, 1)$ model is defined as</p>

<p>$\sigma_{t}^{2} = \alpha_0 + \alpha_1 E_{t-1}^{2} + \beta_1 \sigma_{t-1}^2 \tag{14}$</p>

<p>We can estimate this model using the <code class="highlighter-rouge">garchFit()</code> function in the R library <code class="highlighter-rouge">fGarch</code>.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">fGarch</span><span class="p">)</span><span class="w">
</span><span class="n">heavy_fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">garchFit</span><span class="p">(</span><span class="o">~</span><span class="n">garch</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">MCD_lreturn</span><span class="p">,</span><span class="w"> </span><span class="n">cond.dist</span><span class="o">=</span><span class="s2">"std"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>The parameter <code class="highlighter-rouge">cond.dist=std</code> specifies that the White Noise innovations, $W_{t}$, in the GARCH model are assumed to follow a standardized $t$ -distribution. The estimated results of the GARCH $(1, 1)$ model for the log returns of McDonald’s stock are as follows.</p>

\[\begin{gather}
\hat{X}_{t} = 0.0006 + \hat{\sigma}_{t}\hat{W}_{t}\\ 
\hat{\sigma}_{t}^{2} = 0.0000196 + 0.1173E_{t-1}^{2} + 0.8132\sigma_{t-1}^{2}\\
\hat{W}_{t} \sim t_{4.7}\left(0, \dfrac{4.7}{4.7-2}\right)
\end{gather} \tag{15}\]

<p>All of the parameter estimates in Equation 15, for $\mu$, $\alpha_0$, $\alpha_1$, $\beta_1$, and $v$ in Equations 8, 10, and 14, are statistically significant at a 5% significance level. I have also tested higher orders, but those are not statistically significant at any meaningful statistical level.</p>

<p>Now, we calculate the squared standardized residuals of the fitted model according to Equation $13$ and plot ACF and PACF of these.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">st_resid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">heavy_fit</span><span class="o">@</span><span class="n">residuals</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">heavy_fit</span><span class="o">@</span><span class="n">sigma.t</span><span class="w">

</span><span class="n">par</span><span class="p">(</span><span class="n">mfcol</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="n">acf</span><span class="p">(</span><span class="n">st_resid</span><span class="o">^</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-0.2</span><span class="p">,</span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span><span class="n">pacf</span><span class="p">(</span><span class="n">st_resid</span><span class="o">^</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-0.2</span><span class="p">,</span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">""</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure">
    <img src="/image/VaR/ACF, PACF of squared standardized residuals.png" style="width: 100%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 4.</span> ACF (left) and PACF (right) of the squared standardized residuals from the fitted GARCH $(1, 1)$ model. There is no significant serial dependence anymore.
    </div>
</div>

<p>Since the squared standardized residuals, or the estimated White Noise innovations, $\hat{W}_{t}$, do not show any significant autocorrelation, GARCH $(1, 1)$ is sufficient to capture all the dynamic volatility in  the log returns of McDonald’s stock.</p>

<p>We can also check if the $t$-distribution is a right choice to model the tail behavior of our log returns using qqplots.</p>

<div class="figure">
    <img src="/image/VaR/log returns qqplots with t-dist and Gaussian.png" style="width: 100%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 5.</span> A comparison between the QQplot of log returns with $t$-distribution (left) and the QQplot of log returns with Gaussian distribution (right) shows that the $t$-distribution is better at capturing the heavy-tailed behavior of the log returns of McDonald's stock.
    </div>
</div>

<p>See <a href="#code-4">A4</a> for the code to generate this figure.</p>

<h2 id="daily-var-with-garch">Daily VaR with GARCH</h2>
<p>Now, let’s calculate daily VaR of McDonald’s stock.  With the estimated GARCH $(1, 1)$ model in Equation $15$, the predicted conditional standardized deviation, $\hat{\sigma}_{t}$, right after 2023-01-01 is calculated in R as</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">heavyt_fit</span><span class="p">,</span><span class="w"> </span><span class="n">n.ahead</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="o">$</span><span class="n">standardDeviation</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="m">0.01581185</span><span class="w">
</span></code></pre></div></div>

<p>Let’s add the estimated conditional standard deviation to the estimated results from GARCH $(1, 1)$ in Equation $15$.</p>

\[\begin{gather}
\hat{X}_{t} = 0.0006 + 0.016\hat{W}_{t}\\ 
\hat{W}_{t} \sim t_{4.7}\left(0, \dfrac{4.7}{4.7-2}\right)
\end{gather} \tag{16}\]

<p>We can think of our estimated log return at time $t$, $\hat{X}_{t}$, as a random variable from a non-standardized $t$-distribution with mean of $0.0006$ and variance of $0.016^{2}\times \dfrac{4.7}{4.7-2}$, which could be written as $t_{4.7} \left(0.0006, 0.016^{2}\cdot \dfrac{4.7}{4.7-2} \right)$. It is because  $\hat{W}_{t}$ follows a standardized $t$-distribution with $4.7$ degrees of freedom, $t_{4.7} \left(0, \dfrac{4.7}{4.7-2}\right)$.</p>

<p>But usually, when using $t$-distribution for GARCH modeling, we convert the estimated conditional standard deviation, $\hat{\sigma}_{t}$ by multiplying it by the inverse of the estimated standard deviation of the standardized $t$-distribution, $\sqrt{\dfrac{\hat{v}-2}{\hat{v}}}$, which in our case is $\sqrt{\dfrac{4.7-2}{4.7}}$.</p>

<p>This is to cancel the variance of the standardized $t$-distribution, $\dfrac{v}{v-2}$. There are two reasons for doing so. $1)$ We want our modeled time-series, $\hat{X}_{t}$ to have the same conditional variance as the one estimated from the GARCH model, which in our case is $0.016^{2}$. $2)$ We want to avoid the problem of not being able to calculate the variance of the standardized t distribution when the degrees of freedom are less than or equal to 2. It is because the standardized $t$-distribution has its finite variance, $\dfrac{v}{v-2}$, only when $v&gt;2$.</p>

\[\begin{gather}
\hat{X}_{t} =  0.0006 + 0.016 \cdot \sqrt{\dfrac{4.7-2}{4.7}} \hat{W}_{t} \\
\hat{W}_{t} \sim t_{4.7}\left(0, \dfrac{4.7}{4.7-2}\right) \\
\hat{X}_{t} \sim t_{4.7} \left(0.0006, 0.016^{2} \right)
\end{gather} \tag{17}\]

<p>We can calculate the VaR with a $95\%$ confidence level from the estimated model in Equation $17$ using R as follows.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef</span><span class="p">(</span><span class="n">heavy_fit</span><span class="p">)[</span><span class="s2">"shape"</span><span class="p">]</span><span class="w">
</span><span class="n">adjusted_sd</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">((</span><span class="n">df</span><span class="m">-2</span><span class="p">)</span><span class="o">/</span><span class="n">df</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">heavyt_fit</span><span class="p">,</span><span class="w"> </span><span class="n">n.ahead</span><span class="o">=</span><span class="m">1</span><span class="p">)[</span><span class="s2">"standardDeviation"</span><span class="p">])</span><span class="w">
</span><span class="n">mu</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef</span><span class="p">(</span><span class="n">heavy_fit</span><span class="p">)[</span><span class="s2">"mu"</span><span class="p">]</span><span class="w">

</span><span class="n">mu</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">adjusted_sd</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">qt</span><span class="p">(</span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="p">)</span><span class="w">
</span><span class="m">1</span><span class="w">       </span><span class="m">-0.02384826</span><span class="w">
</span></code></pre></div></div>

<p>The one period (daily) Value at Risk of the log returns of McDonald’s stock over the past five years with a confidence level of $95\%$ is about $-2.385\%$. In other words, we are $95\%$ confident in the fact that our daily maximum expected loss right after 2023-01-01 is less than about $2.385\%$.</p>

<h2 id="appendix">Appendix</h2>

<p><a id="code-1"></a></p>
<h3 id="a1-prices-simple-returns-and-log-returns-plots">A1: Prices, simple returns and log returns plots</h3>
<p>I imported the closing price data of McDonal’s stock for the past five years from Yahoo Finance through <code class="highlighter-rouge">getSymbols</code> function in <code class="highlighter-rouge">quantmod</code> library.</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load Mcdonald's stock data</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">tidyquant</span><span class="p">)</span><span class="w">

</span><span class="n">options</span><span class="p">(</span><span class="s2">"getSymbols.warning4.0"</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span><span class="n">options</span><span class="p">(</span><span class="s2">"getSymbols.yahoo.warning"</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">

</span><span class="n">getSymbols</span><span class="p">(</span><span class="s2">"MCD"</span><span class="p">,</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'2018-01-01'</span><span class="p">,</span><span class="w">
           </span><span class="n">to</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"2023-01-01"</span><span class="p">,</span><span class="n">warnings</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
           </span><span class="n">auto.assign</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">


</span><span class="c1"># price, return, log return into one xts object</span><span class="w">
</span><span class="n">MCD</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">MCD</span><span class="p">[,</span><span class="w"> </span><span class="s2">"MCD.Close"</span><span class="p">]</span><span class="w">
</span><span class="n">MCD_sreturn</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">diff</span><span class="p">(</span><span class="n">MCD</span><span class="p">)</span><span class="o">/</span><span class="n">stats</span><span class="o">::</span><span class="n">lag</span><span class="p">(</span><span class="n">MCD</span><span class="p">)</span><span class="w">  </span><span class="c1">## dplyr::lag(MCD) generates error</span><span class="w">
</span><span class="n">MCD_lreturn</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">diff</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">MCD</span><span class="p">))</span><span class="w">
</span><span class="n">MCD_lreturn</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">na.omit</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">)</span><span class="w">

</span><span class="n">MCD_perform</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">merge</span><span class="p">(</span><span class="n">MCD</span><span class="p">,</span><span class="w"> </span><span class="n">MCD_sreturn</span><span class="p">,</span><span class="w"> </span><span class="n">MCD_lreturn</span><span class="p">)</span><span class="w">
</span><span class="n">colnames</span><span class="p">(</span><span class="n">MCD_perform</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"Price"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Simple_return"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Log_return"</span><span class="p">)</span><span class="w">


</span><span class="c1"># plot the three</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">

</span><span class="n">autoplot</span><span class="p">(</span><span class="n">MCD_perform</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s2">"Year"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_date</span><span class="p">(</span><span class="n">date_breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"1 year"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">facet_grid</span><span class="p">(</span><span class="n">Series</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="o">=</span><span class="s2">"free_y"</span><span class="p">,</span><span class="w">
             </span><span class="n">switch</span><span class="o">=</span><span class="s2">"y"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">strip.background</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_blank</span><span class="p">(),</span><span class="w">
        </span><span class="n">panel.background</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_blank</span><span class="p">(),</span><span class="w">
        </span><span class="n">panel.grid.major</span><span class="w"> </span><span class="o">=</span><span class="w">  </span><span class="n">element_line</span><span class="p">(</span><span class="n">colour</span><span class="o">=</span><span class="s2">"#E7E7E7"</span><span class="p">),</span><span class="w">
        </span><span class="n">panel.grid.minor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_line</span><span class="p">(</span><span class="n">colour</span><span class="o">=</span><span class="s2">"#E7E7E7"</span><span class="p">),</span><span class="w">
        </span><span class="n">axis.line</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_line</span><span class="p">(</span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"black"</span><span class="p">),</span><span class="w">
        </span><span class="n">strip.placement</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"outside"</span><span class="p">,</span><span class="w">
        </span><span class="n">strip.text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">14</span><span class="p">),</span><span class="w">
        </span><span class="n">axis.title.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">14</span><span class="p">),</span><span class="w">
        </span><span class="n">axis.text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="m">14</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">"#1F77B4"</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><a id="code-2"></a></p>
<h3 id="a2-empirical-var-plot">A2: Empirical VaR plot</h3>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># plot 1-period empirical VaR with confidence of 95%</span><span class="w">
</span><span class="n">VaR</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">quantile</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">,</span><span class="w"> </span><span class="m">0.05</span><span class="p">)</span><span class="w">

</span><span class="n">hist</span><span class="p">(</span><span class="n">returns</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"#1F77B4"</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Daily Log Return"</span><span class="p">,</span><span class="w">
     </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Frequency"</span><span class="p">,</span><span class="w"> </span><span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">650</span><span class="p">))</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">VaR</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"#D62728"</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">legend</span><span class="p">(</span><span class="s2">"topright"</span><span class="p">,</span><span class="w"> </span><span class="n">legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="s2">"VaR (95% confidence): "</span><span class="p">,</span><span class="w"> </span><span class="nf">round</span><span class="p">(</span><span class="n">VaR</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w">
                                   </span><span class="s2">"%"</span><span class="p">),</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"#D62728"</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">cex</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.8</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><a id="code-3"></a></p>
<h3 id="a3-acf-and-pacf-plot">A3: ACF and PACF plot</h3>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MCD_lreturn_sq</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">MCD_lreturn</span><span class="o">^</span><span class="m">2</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfcol</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="n">acf</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">,</span><span class="w"> </span><span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-0.2</span><span class="p">,</span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"ACF of log returns"</span><span class="p">)</span><span class="w">
</span><span class="n">acf</span><span class="p">(</span><span class="n">MCD_lreturn_sq</span><span class="p">,</span><span class="w"> </span><span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-0.2</span><span class="p">,</span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"ACF of squared log returns"</span><span class="p">)</span><span class="w">
</span><span class="n">pacf</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">,</span><span class="w"> </span><span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-0.2</span><span class="p">,</span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"PACF of log returns"</span><span class="p">)</span><span class="w">
</span><span class="n">pacf</span><span class="p">(</span><span class="n">MCD_lreturn_sq</span><span class="p">,</span><span class="w"> </span><span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-0.2</span><span class="p">,</span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"PACF of squared log returns"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><a id="code-4"></a></p>
<h3 id="a4-qqplots-with-two-distributions">A4: QQplots with two distributions</h3>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">par</span><span class="p">(</span><span class="n">mfcol</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="c1">## qqplot with t-dist</span><span class="w">
</span><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef</span><span class="p">(</span><span class="n">heavy_fit</span><span class="p">)[</span><span class="s2">"shape"</span><span class="p">]</span><span class="w">
</span><span class="n">theo_t_quantiles</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sort</span><span class="p">(</span><span class="n">coef</span><span class="p">(</span><span class="n">heavy_fit</span><span class="p">)[</span><span class="s2">"mu"</span><span class="p">]</span><span class="o">+</span><span class="w"> 
                           </span><span class="n">heavy_fit</span><span class="o">@</span><span class="n">sigma.t</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">qt</span><span class="p">(</span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="o">=</span><span class="nf">length</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">)),</span><span class="w"> </span><span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">))</span><span class="w">

</span><span class="n">plot</span><span class="p">(</span><span class="n">theo_t_quantiles</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="p">(</span><span class="n">coredata</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">)),</span><span class="w"> 
     </span><span class="n">main</span><span class="o">=</span><span class="s2">"log returns QQ plot with t-dist"</span><span class="p">,</span><span class="w"> 
     </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"quantiles of theoretical t distribution"</span><span class="p">,</span><span class="w"> 
     </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"log returns"</span><span class="p">)</span><span class="w">

</span><span class="c1">## qqline with t-dist</span><span class="w">
</span><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">quantile</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">,</span><span class="w"> </span><span class="n">probs</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0.25</span><span class="p">,</span><span class="w"> </span><span class="m">0.75</span><span class="p">))</span><span class="w">
</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef</span><span class="p">(</span><span class="n">heavy_fit</span><span class="p">)[</span><span class="s2">"mu"</span><span class="p">]</span><span class="w">  </span><span class="o">+</span><span class="w"> 
  </span><span class="n">quantile</span><span class="p">(</span><span class="n">heavy_fit</span><span class="o">@</span><span class="n">sigma.t</span><span class="w"> </span><span class="o">*</span><span class="w"> 
             </span><span class="n">qt</span><span class="p">(</span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="o">=</span><span class="nf">length</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">)),</span><span class="w"> </span><span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">),</span><span class="w"> </span><span class="n">probs</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0.25</span><span class="p">,</span><span class="w"> </span><span class="m">0.75</span><span class="p">))</span><span class="w">

</span><span class="n">slope</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">diff</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">/</span><span class="n">diff</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="w">
</span><span class="n">intercept</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">slope</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w">

</span><span class="n">abline</span><span class="p">(</span><span class="n">intercept</span><span class="p">,</span><span class="w"> </span><span class="n">slope</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span><span class="w">

</span><span class="c1">## qqplot and qqline with Gaussian</span><span class="w">
</span><span class="n">qqnorm</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">,</span><span class="w"> 
       </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"log returns QQ plot with Gaussian"</span><span class="p">,</span><span class="w"> 
       </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"quantiles of theoretical Gaussian distribution"</span><span class="p">,</span><span class="w"> 
       </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"log returns"</span><span class="p">)</span><span class="w">

</span><span class="n">qqline</span><span class="p">(</span><span class="n">MCD_lreturn</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>]]></content><author><name></name></author><category term="all" /><category term="ts" /><summary type="html"><![CDATA[In finance, Value at Risk (VaR) is a crucial metric for assessing the risk of an investment portfolio. VaR provides an estimate of rare but potential loss an investment may face over a specific time horizon and confidence level. To calculate VaR, it’s necessary to have an understanding of the underlying distribution of investment returns.]]></summary></entry></feed>